{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_weights.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db7QkgBLYZi0",
        "colab_type": "text"
      },
      "source": [
        "# This file is used to train the weights that is used in our CNN\n",
        "We kept it as a ipynb file, as it was trained in google colab.\n",
        "\n",
        "We start by connecting the notebook to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5jUBGE8YdNS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "bace47e0-152e-423b-ef3a-ea968008d210"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "import numpy as np\n",
        "\n",
        "from unet import *\n",
        "from augmentation import *\n",
        "from training_helpers import test_weights"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SMKqxvCZdcZ",
        "colab_type": "text"
      },
      "source": [
        "### Once we have imported all necessary files we start by making a model and print a summary of how it is constructed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXHCVC9PrS4A",
        "colab_type": "code",
        "outputId": "e4276370-e7c0-4ce0-983f-bdc15f0a911c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1686
        }
      },
      "source": [
        "model = get_unet()\n",
        "model.summary()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 256, 256, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 256, 256, 64) 1792        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 256, 256, 64) 36928       conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 128, 128, 64) 0           conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 128, 128, 128 73856       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 128, 128, 128 147584      conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 64, 64, 128)  0           conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 64, 64, 256)  295168      max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 64, 64, 256)  590080      conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 32, 32, 256)  0           conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 32, 32, 512)  1180160     max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 32, 32, 512)  2359808     conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 32, 32, 512)  0           conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 16, 16, 512)  0           dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 16, 16, 1024) 4719616     max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 16, 16, 1024) 9438208     conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 16, 16, 1024) 0           conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2D)  (None, 32, 32, 1024) 0           dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 32, 32, 512)  2097664     up_sampling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 32, 32, 1024) 0           dropout_1[0][0]                  \n",
            "                                                                 conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 32, 32, 512)  4719104     concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 32, 32, 512)  2359808     conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_2 (UpSampling2D)  (None, 64, 64, 512)  0           conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 64, 64, 256)  524544      up_sampling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 64, 64, 512)  0           conv2d_6[0][0]                   \n",
            "                                                                 conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 64, 64, 256)  1179904     concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 64, 64, 256)  590080      conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_3 (UpSampling2D)  (None, 128, 128, 256 0           conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 128, 128, 128 131200      up_sampling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 128, 128, 256 0           conv2d_4[0][0]                   \n",
            "                                                                 conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 128, 128, 128 295040      concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 128, 128, 128 147584      conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_4 (UpSampling2D)  (None, 256, 256, 128 0           conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 256, 256, 64) 32832       up_sampling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 256, 256, 128 0           conv2d_2[0][0]                   \n",
            "                                                                 conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 256, 256, 64) 73792       concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 256, 256, 64) 36928       conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 256, 256, 2)  1154        conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 256, 256, 1)  3           conv2d_23[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 31,032,837\n",
            "Trainable params: 31,032,837\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/unet.py:52: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"co...)`\n",
            "  model = Model(input = inputs, output = conv10)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDowOx0Saea6",
        "colab_type": "text"
      },
      "source": [
        "### We now declare variables to tell the CNN where it can retrieve both original and ground truth images, as well as where to store the weights file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKMx9dkQrS4F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#using google drive to fetch images\n",
        "drive_dir = \"drive/My Drive/\"\n",
        "#variables used to find folders of images\n",
        "train_dir = drive_dir+\"train/\"\n",
        "weights_dir = train_dir+\"weights/\"\n",
        "image_folder = 'originals'\n",
        "gt_folder = 'whites'\n",
        "#this is the name the weights file will get once it is trained\n",
        "weights_filename ='weights.hdf5'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzVgKV2Obykk",
        "colab_type": "text"
      },
      "source": [
        "### We declare variables that is used to create a larger image dataset through augmentation. Note that as the image dataset is less suited for augmentation we do not generate as large variety of images as we might have with another dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VP1FgrRvrS4D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#augmentation variables\n",
        "generator_variables = dict(rotation_range=0.2,\n",
        "                           width_shift_range=0.1,\n",
        "                           height_shift_range=0.1,\n",
        "                           zoom_range=0.2,\n",
        "                           horizontal_flip=True,\n",
        "                           vertical_flip = False,\n",
        "                           fill_mode='reflect')\n",
        "#variables used to decide how long the model is trained\n",
        "batch_size = 2\n",
        "steps_per_epoch = 300\n",
        "epochs=10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iw009FpzcYO8",
        "colab_type": "text"
      },
      "source": [
        "### We then train the model. The images are retrieved from the drive. With 300 steps per epoch, batch size of 2 and 10 epochs it takes approximately 30 minutes to train the model in Colab using GPU accelerator. We can also train the model over again several times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtPCfoV6rS4N",
        "colab_type": "code",
        "outputId": "e5fad5d3-8f01-4162-98e2-9666f6dcb5d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        }
      },
      "source": [
        "\n",
        "train_generator = trainGenerator(batch_size,train_dir,image_folder,gt_folder,generator_variables)\n",
        "#model.load_weights(weights_dir + weights_filename)\n",
        "checkpoint = ModelCheckpoint(weights_dir + weights_filename, monitor='loss', verbose=1, save_best_only=True)\n",
        "model.fit_generator(train_generator,steps_per_epoch=steps_per_epoch,epochs=epochs, callbacks=[checkpoint])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "Found 50 images belonging to 1 classes.\n",
            "Found 50 images belonging to 1 classes.\n",
            "300/300 [==============================] - 195s 650ms/step - loss: 0.0897 - acc: 0.9667\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.08966, saving model to drive/My Drive/train/weights/weights.hdf5\n",
            "Epoch 2/10\n",
            "300/300 [==============================] - 192s 640ms/step - loss: 0.0950 - acc: 0.9653\n",
            "\n",
            "Epoch 00002: loss did not improve from 0.08966\n",
            "Epoch 3/10\n",
            "300/300 [==============================] - 191s 638ms/step - loss: 0.0879 - acc: 0.9679\n",
            "\n",
            "Epoch 00003: loss improved from 0.08966 to 0.08785, saving model to drive/My Drive/train/weights/weights.hdf5\n",
            "Epoch 4/10\n",
            "300/300 [==============================] - 194s 646ms/step - loss: 0.0872 - acc: 0.9682\n",
            "\n",
            "Epoch 00004: loss improved from 0.08785 to 0.08716, saving model to drive/My Drive/train/weights/weights.hdf5\n",
            "Epoch 5/10\n",
            "300/300 [==============================] - 193s 642ms/step - loss: 0.0840 - acc: 0.9692\n",
            "\n",
            "Epoch 00005: loss improved from 0.08716 to 0.08396, saving model to drive/My Drive/train/weights/weights.hdf5\n",
            "Epoch 6/10\n",
            "300/300 [==============================] - 193s 645ms/step - loss: 0.0877 - acc: 0.9676\n",
            "\n",
            "Epoch 00006: loss did not improve from 0.08396\n",
            "Epoch 7/10\n",
            "300/300 [==============================] - 193s 642ms/step - loss: 0.0817 - acc: 0.9712\n",
            "\n",
            "Epoch 00007: loss improved from 0.08396 to 0.08169, saving model to drive/My Drive/train/weights/weights.hdf5\n",
            "Epoch 8/10\n",
            "300/300 [==============================] - 195s 650ms/step - loss: 0.0815 - acc: 0.9701\n",
            "\n",
            "Epoch 00008: loss improved from 0.08169 to 0.08150, saving model to drive/My Drive/train/weights/weights.hdf5\n",
            "Epoch 9/10\n",
            "300/300 [==============================] - 195s 650ms/step - loss: 0.0785 - acc: 0.9717\n",
            "\n",
            "Epoch 00009: loss improved from 0.08150 to 0.07853, saving model to drive/My Drive/train/weights/weights.hdf5\n",
            "Epoch 10/10\n",
            "300/300 [==============================] - 195s 650ms/step - loss: 0.0860 - acc: 0.9675\n",
            "\n",
            "Epoch 00010: loss did not improve from 0.07853\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff4817652b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jjm5vF0Hc5vM",
        "colab_type": "text"
      },
      "source": [
        "### Once we have  a trained model, we use methods in training_helpers.py to save predictions on a few images, as well as calculate metrics such as accuracy, and f1 to give a sense of how well the model performs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KmmtPqjrS4P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "outputId": "0403bfec-6804-4951-9309-d2165ca178db"
      },
      "source": [
        "#variables to tell where test images are located and where prediction images should be stored\n",
        "test_dir = train_dir+\"testing/\"\n",
        "test_image_dir = test_dir+\"test_images/\"\n",
        "test_gt_dir = test_dir+\"test_gt/\"\n",
        "prediction_dir = test_dir+\"predictions/\"\n",
        "#we then declare variables for the standardized names of the images in the test folders as well as how many of them we want\n",
        "test_image_name = \"test%d.png\"\n",
        "test_gt_name = \"test_gt%d.png\"\n",
        "first_image = 1\n",
        "last_image = 6\n",
        "\n",
        "#We then run a method that stores predictions and returns different metrics\n",
        "accuracy, recall, precision, f1 = test_weights(weights_dir+weights_filename,prediction_dir, test_image_dir, \n",
        "                                               test_image_name,test_gt_dir, test_gt_name, first_image,last_image,\n",
        "                                               treshold=0.2)\n",
        "\n",
        "print(\"Accuracy: \"+str(accuracy)+\"\\n\"+\n",
        "     \"Recall: \"+str(recall)+\"\\n\"+\n",
        "     \"Precision: \"+str(precision)+\"\\n\"+\n",
        "     \"f1: \"+str(f1)+\"\\n\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/unet.py:52: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"co...)`\n",
            "  model = Model(input = inputs, output = conv10)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-aa3ea546bd01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m accuracy, recall, precision, f1 = test_weights(weights_dir+weights_filename,prediction_dir, test_image_dir, \n\u001b[1;32m     13\u001b[0m                                                \u001b[0mtest_image_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_gt_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_gt_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst_image\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlast_image\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                                                treshold=0.2)\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m print(\"Accuracy: \"+str(accuracy)+\"\\n\"+\n",
            "\u001b[0;32m/content/training_helpers.py\u001b[0m in \u001b[0;36mtest_weights\u001b[0;34m(weights, pred_dir, image_dir, image_name, gt_dir, gt_name, start, end, treshold)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;31m#load the testing images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_image_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m#predict on the testing images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/training_helpers.py\u001b[0m in \u001b[0;36mload_image_set\u001b[0;34m(directory, name, f, t)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;31m#loads a set of images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_image_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0mtest_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtest_images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/training_helpers.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;31m#loads a set of images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_image_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0mtest_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtest_images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/training_helpers.py\u001b[0m in \u001b[0;36mload_image\u001b[0;34m(infilename)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;31m#method loads an image, and converts it so that it is ready to be used for prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_for_CNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfTHXFgWkTGQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}